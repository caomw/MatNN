function [ net ] = InitNet(net_param, data_set, test_set)
net.net_param = net_param;
batch_sz = net_param.train_param.batch_size;
test_batch_sz = net_param.test_param.batch_size;
layer_num = length(net_param.architecture)/2;
layers = cell(layer_num, 1); % parameters of the layer
layers_train = cell(layer_num, 1); % memory for training, including derivative, derivative history of trainable parameters
feature_train = cell(layer_num + 1, 1); % input features. 
for i = 1:2:layer_num * 2
    switch net_param.architecture{i}
        case 'input',
            layers{i} = struct('batch_sz', net_param.train_param.batch_size, 'batch_id', 1,...
                               'batch_num', size(data_set.data, 2) / batch_sz,...
                               'test_param', struct('batch_sz', test_batch_sz, ...
                               'batch_id', 1, 'batch_num', size(test_set.data, 2) / test_batch_sz), ...
                               'forward', @InputForward);
            layers_train{i} = [];
            feature_train{ceil(i/2)} = struct('input', struct('data', data_set.data,...
                                              'label', data_set.label),...
                                              'test_input', struct('data', test_set.data,...
                                              'label', test_set.label)); % load all the training samples/labels into feature_train
        case 'fc',
            sz = net_param.architecture{i+1};
            layer = struct('W', 0.1*randn(sz(2),sz(1)),...
                           'b', 0.1*ones(sz(2),1),...
                           'weight_decay', net_param.train_param.weight_decay,...
                           'forward', @FCForward,...
                           'backward', @FCBackward,...
                           'update', @FCUpdate);
            layer_train = struct('W_hist',  zeros(sz(2),sz(1)),...
                                 'W_diff',  zeros(sz(2),sz(1)),...
                                 'b_hist', zeros(sz(2),1),...
                                 'b_diff', zeros(sz(2),1));
            layers{ceil(i/2)} = layer;
            layers_train{ceil(i/2)} = layer_train;
            feature_train{ceil(i/2)} = struct('input', struct('data', zeros(sz(1), batch_sz),...
                                              'data_diff', zeros(sz(1), batch_sz)));
        case 'relu',
            sz = net_param.architecture{i+1};
            layer = struct('forward', @ReLUForward, 'backward', @ReLUBackward, 'update', @ReLUUpdate);
            layers{ceil(i/2)} = layer;
            layers_train{ceil(i/2)} = [];
            feature_train{ceil(i/2)} = struct('input', struct('data', zeros(sz(1), batch_sz),...
                                              'data_diff', zeros(sz(1), batch_sz)));
        case 'euclidean',
            sz = net_param.architecture{i+1};
            layer = struct('forward', @EuclideanForward, 'backward', @EuclideanBackward);
            layers{ceil(i/2)} = layer;
            layers_train{ceil(i/2)} = [];
            feature_train{ceil(i/2)} = struct('input', struct('data',zeros(sz(1), batch_sz), 'label',zeros(sz(1), batch_sz),...
                                              'data_diff', zeros(sz(1), batch_sz)));
        otherwise,
            break;
    end
end
feature_train{end} = struct('input', struct('data',zeros(sz(1), batch_sz), 'label',zeros(sz(1), batch_sz),...
                                              'data_diff', zeros(sz(1), batch_sz)));
net.layers = layers;
net.layers_train = layers_train;
net.feature_train = feature_train;
end

